{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import fitz\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import math\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files\n",
    "def read_txt(file_path): #reading .txt files\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        f.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "def read_pdf(file_path): #reading .pdf files\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def read_file(file_path): #one function to read all supported file types\n",
    "    _, file_extention = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extention.lower() == '.txt':\n",
    "        return read_txt(file_path=file_path)\n",
    "    elif file_extention.lower() == '.pdf':\n",
    "        return read_pdf(file_path=file_path)\n",
    "    else:\n",
    "        return \"Unsupported file type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the data as a corpus\n",
    "def store_data(data_dir):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            data.append([file, read_file(os.path.join(root, file))])\n",
    "    return data\n",
    "\n",
    "\n",
    "def chunk_data(data, max_chunk_length=500):\n",
    "    print(\"Chunking the data...\")\n",
    "    chunks = []\n",
    "    for file in data:\n",
    "        content = file[1]\n",
    "        pdf_length = len(content)\n",
    "        for i in range(pdf_length//max_chunk_length + 5):\n",
    "            if content == \"\":\n",
    "                break\n",
    "            contentL = len(content)\n",
    "            temp_length = min(max_chunk_length, contentL)\n",
    "            temp_chunk = content[:temp_length]\n",
    "\n",
    "            dotIndex = temp_chunk.rfind(\". \")\n",
    "            dotIndex2 = temp_chunk.rfind(\".\\n\")\n",
    "            qIndex = temp_chunk.rfind(\"? \")\n",
    "            qIndex2 = temp_chunk.rfind(\"?\\n\")\n",
    "            excIndex = temp_chunk.rfind(\"! \")\n",
    "            excIndex2 = temp_chunk.rfind(\"!\\n\")\n",
    "            entIndex = temp_chunk.rfind(\"\\n\")\n",
    "\n",
    "            lastIndex = max(dotIndex, dotIndex2, qIndex, qIndex2, excIndex, excIndex2, entIndex)\n",
    "            chunk = [file[0], content[:lastIndex]]\n",
    "            content = content.replace(chunk[1], \"\", 1)\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding the chunks\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    print(\"Generating embeddings...\")\n",
    "    texts = [chunk[1] for chunk in chunks]\n",
    "    chunk_embeddings = model.encode(texts)\n",
    "    return list(zip([chunk[0] for chunk in chunks], chunk_embeddings))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the embeddings in a vector DB\n",
    "class VectorDB:\n",
    "    def __init__(self):\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_embeddings(self, embeddings: List[Tuple[str, List[float]]]):\n",
    "        for source, embedding in embeddings:\n",
    "            self.embeddings.append(embedding)\n",
    "            self.metadata.append({\"source\": source})\n",
    "\n",
    "    def _euclidean_distance(self, v1, v2):\n",
    "        score = 0\n",
    "        for i in range(len(v1)):\n",
    "            score += (v1[i] - v2[i])**2\n",
    "        return score * (1 / len(v1))\n",
    "\n",
    "    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Tuple[float, dict]]:\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "\n",
    "        distances = [self._euclidean_distance(query_embedding, emb) for emb in self.embeddings]\n",
    "        \n",
    "        # Sort distances and get top_k (smallest distances)\n",
    "        sorted_results = sorted(enumerate(zip(distances, self.metadata)), key=lambda x: x[1][0])\n",
    "        return [(distance, metadata, index) for index, (distance, metadata) in sorted_results[:top_k]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        with open(filepath, 'w') as f:\n",
    "            for embedding, metadata in zip(self.embeddings, self.metadata):\n",
    "                embedding_str = ','.join(map(str, embedding))\n",
    "                f.write(f\"{metadata['source']}|{embedding_str}\\n\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        vector_store = cls()\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                source, embedding_str = line.strip().split('|')\n",
    "                embedding = list(map(float, embedding_str.split(',')))\n",
    "                vector_store.embeddings.append(embedding)\n",
    "                vector_store.metadata.append({\"source\": source})\n",
    "        return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Files...\n",
      "Chunking the data...\n",
      "Source: 02.Preparation.pdf, Distance: 0.0037\n",
      "02.Preparation.pdf\n",
      "Modern C++\n",
      "Programming\n",
      "2. Preparation\n",
      "Federico Busato\n",
      "2024-04-10\n",
      "Table of Contents\n",
      "1 Books and References\n",
      "2 Slide Legend\n",
      "3 What Editor/ IDE/Compiler Should I Use?\n",
      "4 How to compile?\n",
      "5 Hello World\n",
      "I/O Stream\n",
      "1/22\n",
      "Books and\n",
      "References\n",
      "Suggested Books\n",
      "Programming and Principles\n",
      "using C++ (2nd)\n",
      "B. Stroustrup, 2014\n",
      "Professional C++ (5th)\n",
      "S. J. Kleper, N. A. Solter, 2021\n",
      "Absolute C++ (6th)\n",
      "W. Savitch, 2015\n",
      "2/22\n",
      "More Advanced Books\n",
      "Effective Modern C++\n",
      "S. Meyer, 2014\n",
      "Embracing Modern C++\n",
      "Safely\n",
      "J\n",
      "\n",
      "Source: modern-cpp.pdf, Distance: 0.0040\n",
      "modern-cpp.pdf\n",
      ". Savitch, 2015\n",
      "2/22\n",
      "More Advanced Books\n",
      "Eﬀective Modern C++\n",
      "S. Meyer, 2014\n",
      "Embracing Modern C++\n",
      "Safely\n",
      "J. Lakos, V. Romeo, R.\n",
      "Khlebnikov, A. Meredith, 2021\n",
      "Beautiful C++: 30 Core\n",
      "Guidelines for Writing Clean,\n",
      "Safe, and Fast Code\n",
      "J. G. Davidson, K. Gregory, 2021\n",
      "3/22\n",
      "References\n",
      "1/3\n",
      "(Un)oﬃcial C++ reference:*\n",
      "• en.cppreference.com\n",
      "• C++ Standard Draft\n",
      "Tutorials:\n",
      "• www.learncpp.com\n",
      "• www.tutorialspoint.com/cplusplus\n",
      "• en.wikibooks.org/wiki/C++\n",
      "• yet another insignificant...programming notes\n",
      "\n",
      "Source: modern-cpp.pdf, Distance: 0.0042\n",
      "modern-cpp.pdf\n",
      "\n",
      "gramming language is by writing pro-\n",
      "grams in it”\n",
      "Dennis Ritchie\n",
      "Creator of the C programming language\n",
      "56/56\n",
      "Modern C++\n",
      "Programming\n",
      "1a. Preparation\n",
      "Federico Busato\n",
      "2024-03-29\n",
      "Table of Contents\n",
      "1 Books and References\n",
      "2 Slide Legend\n",
      "3 What Editor/ IDE/Compiler Should I Use?\n",
      "4 How to compile?\n",
      "5 Hello World\n",
      "I/O Stream\n",
      "1/22\n",
      "Books and\n",
      "References\n",
      "Suggested Books\n",
      "Programming and Principles\n",
      "using C++ (2nd)\n",
      "B. Stroustrup, 2014\n",
      "Professional C++ (5th)\n",
      "S. J. Kleper, N. A. Solter, 2021\n",
      "Absolute C++ (6th)\n",
      "W\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vector_store = VectorDB()\n",
    "    print(\"Reading Files...\")\n",
    "    data = store_data('Data')\n",
    "    chunked_data = chunk_data(data=data)\n",
    "\n",
    "    if not(os.path.exists(\"vector_store.txt\")):    \n",
    "        embeddings = generate_embeddings(chunked_data)\n",
    "        vector_store.add_embeddings(embeddings=embeddings)\n",
    "        print(f\"Number of embeddings stored: {len(vector_store)}\")\n",
    "        vector_store.save('vector_store.txt')\n",
    "\n",
    "\n",
    "    else:\n",
    "        shouldUpdate = input(\"Do you wish to update the DB? (y/n)\")\n",
    "        if shouldUpdate == 'y':\n",
    "            embeddings = generate_embeddings(chunked_data)\n",
    "            vector_store.add_embeddings(embeddings=embeddings)\n",
    "            print(f\"Number of embeddings stored: {len(vector_store)}\")\n",
    "            vector_store.save('vector_store.txt')\n",
    "\n",
    "        else:\n",
    "            vector_store = VectorDB.load('vector_store.txt')\n",
    "\n",
    "\n",
    "    query = input('->')\n",
    "    query_embedding = model.encode([query])[0]\n",
    "\n",
    "    results = vector_store.search(query_embedding, top_k=3)\n",
    "\n",
    "    for distance, metadata, chunk_num in results:\n",
    "            print(f\"Source: {metadata['source']}, Distance: {distance:.4f}\")\n",
    "            print('\\n'.join(chunked_data[chunk_num]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_data[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
